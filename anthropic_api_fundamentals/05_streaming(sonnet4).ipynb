{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Streaming Responses\n",
    "\n",
    "In this lesson, we'll learn how to stream responses from Claude using the Anthropic API. Streaming allows you to receive and display partial responses as they're generated, creating a more interactive user experience.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand when and why to use streaming\n",
    "- Learn how to implement streaming with the Anthropic Python SDK\n",
    "- Handle streaming responses properly\n",
    "- Implement error handling for streaming requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set up our API client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (0.52.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (1.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (from anthropic) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (from anthropic) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (from anthropic) (2.11.5)\n",
      "Requirement already satisfied: sniffio in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (from anthropic) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/george/repos/anthropic-courses/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install anthropic python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Anthropic client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Anthropic client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Streaming\n",
    "\n",
    "Let's start with a simple streaming example. The key is to use the `stream=True` parameter in your message creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting simple streaming example...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r_/st8p53nx0kvgy3_5jx012p940000gn/T/ipykernel_67023/2529924853.py:20: DeprecationWarning: The model 'claude-3-sonnet-20240229' is deprecated and will reach end-of-life on July 21st, 2025.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  simple_stream_example()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a short story about a robot learning to paint:\n",
      "\n",
      "Brushstrokes of Discovery\n",
      "\n",
      "The robotics lab was quiet, except for the faint whirring of servomotors and cooling fans. In the corner, an advanced robot prototype stood motionless, awaiting its next directive from the team of engineers. Today would be different though - today it would be given an unexpected task far outside its normal parameters. \n",
      "\n",
      "\"Okay, let's try this,\" Dr. Hendricks said as she punched in a new sequence of code. \"We're going to upload some basic painting skills to see how the AI adapts.\"\n",
      "\n",
      "There were a few moments of data transferring and system recalibrations. Then, a light on the robot's chassis began blinking as the skills slowly integrated into its neural networks. Its arm assemblies twitched and reconfigured, picking up the paintbrush resting on the easel. \n",
      "\n",
      "At first the strokes were crude, jerky motions leaving harsh lines across the blank canvas. The robotic arm lacked the dexterity and subtlety to paint with any nuance or control. But then something seemed to shift - almost like it had an intuitive breakthrough. The motions became smoother, more fluid and purposeful. Blends of colors emerged, filling in shapes and forms across the painting surface.\n",
      "\n",
      "The engineers watched in awe as an abstract, almost impressionistic image bloomed into being through the robot's artificial hand. A vivid landscape of swirling brushstrokes and vibrant hues that seemed to pulse with an energy and spontaneity they never could have programmed directly.\n",
      "\n",
      "When it was finished, the robot froze again, awaiting its next instructions. But the team realized they had stumbled onto something far bigger. This was just the first baby step towards an artificial intelligence that could not just follow instructions precisely, but interpret them creatively. A whole new frontier of technological innovation had suddenly swung open before them.\n",
      "\n",
      "‚úÖ Streaming completed!\n"
     ]
    }
   ],
   "source": [
    "def simple_stream_example():\n",
    "    \"\"\"Basic streaming example\"\"\"\n",
    "    print(\"üöÄ Starting simple streaming example...\\n\")\n",
    "    \n",
    "    with client.messages.stream(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1000,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Write a short story about a robot learning to paint.\"\n",
    "            }\n",
    "        ]\n",
    "    ) as stream:\n",
    "        for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\n‚úÖ Streaming completed!\")\n",
    "\n",
    "simple_stream_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Different Event Types\n",
    "\n",
    "When streaming, you can access different types of events and data. Let's explore the various event types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Detailed streaming example with event handling...\n",
      "\n",
      "üì® Message started - ID: msg_01KiPMxWfFHc4jm9vfwX8sDv\n",
      "   Model: claude-3-haiku-20240307\n",
      "   Usage: Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=15, output_tokens=4, server_tool_use=None, service_tier='standard')\n",
      "\n",
      "üìù Content block started - Type: text\n",
      "\n",
      "Quantum computing is a type of computing that uses the principles of quantum mechanics, the fundamental laws of nature at the smallest scale, to perform calculations.\n",
      "\n",
      "Here's a simple explanation:\n",
      "\n",
      "1. Bits vs. Qubits:\n",
      "   - In classical computers, information is stored and processed using bits, which can be either 0 or 1.\n",
      "   - In quantum computers, the basic unit of information is called a qubit (quantum bit). Qubits can exist in a superposition of 0 and 1 at the same time.\n",
      "\n",
      "2. Superposition:\n",
      "   - Qubits can be in a superposition of 0 and 1, meaning they can represent both values simultaneously.\n",
      "   - This allows quantum computers to explore multiple possibilities in parallel, potentially leading to faster problem-solving.\n",
      "\n",
      "3. Entanglement:\n",
      "   - Qubits can become \"entangled,\" meaning their states are linked, even if they are physically separated.\n",
      "   - Entanglement allows quantum computers to perform certain calculations much more efficiently than classical computers.\n",
      "\n",
      "4. Quantum Algorithms:\n",
      "   - Quantum computers can run specialized algorithms that take advantage of quantum mechanics to solve certain problems much faster than classical computers.\n",
      "   - Examples include Shor's algorithm for factoring large numbers and Grover's algorithm for searching databases.\n",
      "\n",
      "5. Applications:\n",
      "   - Quantum computers have the potential to solve certain problems, such as cryptography, drug discovery, and optimization, much faster than classical computers.\n",
      "   - However, building a large-scale, practical quantum computer is still a significant challenge, and the technology is currently in the research and development stage.\n",
      "\n",
      "The key idea behind quantum computing is to leverage the unique properties of quantum mechanics, like superposition and entanglement, to perform computations in a fundamentally different way than classical computers, potentially leading to breakthroughs in various fields.\n",
      "\n",
      "üìÑ Content block completed\n",
      "\n",
      "üèÅ Message completed\n",
      "\n",
      "\n",
      "üìã Full response length: 1902 characters\n",
      "‚úÖ Detailed streaming completed!\n"
     ]
    }
   ],
   "source": [
    "def detailed_stream_example():\n",
    "    \"\"\"Example showing different streaming events\"\"\"\n",
    "    print(\"üîç Detailed streaming example with event handling...\\n\")\n",
    "    \n",
    "    full_response = \"\"\n",
    "    \n",
    "    with client.messages.stream(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=500,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Explain quantum computing in simple terms.\"\n",
    "            }\n",
    "        ]\n",
    "    ) as stream:\n",
    "        # Handle the stream events\n",
    "        for event in stream:\n",
    "            if event.type == \"message_start\":\n",
    "                print(f\"üì® Message started - ID: {event.message.id}\")\n",
    "                print(f\"   Model: {event.message.model}\")\n",
    "                print(f\"   Usage: {event.message.usage}\\n\")\n",
    "                \n",
    "            elif event.type == \"content_block_start\":\n",
    "                print(f\"üìù Content block started - Type: {event.content_block.type}\\n\")\n",
    "                \n",
    "            elif event.type == \"content_block_delta\":\n",
    "                if hasattr(event.delta, 'text'):\n",
    "                    text_chunk = event.delta.text\n",
    "                    full_response += text_chunk\n",
    "                    print(text_chunk, end=\"\", flush=True)\n",
    "                    \n",
    "            elif event.type == \"content_block_stop\":\n",
    "                print(\"\\n\\nüìÑ Content block completed\")\n",
    "                \n",
    "            elif event.type == \"message_delta\":\n",
    "                if hasattr(event.delta, 'usage'):\n",
    "                    print(f\"\\nüìä Usage update: {event.delta.usage}\")\n",
    "                    \n",
    "            elif event.type == \"message_stop\":\n",
    "                print(\"\\nüèÅ Message completed\")\n",
    "    \n",
    "    print(f\"\\n\\nüìã Full response length: {len(full_response)} characters\")\n",
    "    print(\"‚úÖ Detailed streaming completed!\")\n",
    "\n",
    "detailed_stream_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Streaming\n",
    "\n",
    "For applications that need non-blocking operations, you can use async streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize async client\n",
    "async_client = anthropic.AsyncAnthropic(\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    ")\n",
    "\n",
    "async def async_stream_example():\n",
    "    \"\"\"Async streaming example\"\"\"\n",
    "    print(\"‚ö° Starting async streaming example...\\n\")\n",
    "    \n",
    "    async with async_client.messages.stream(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=500,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Write a haiku about streaming data.\"\n",
    "            }\n",
    "        ]\n",
    "    ) as stream:\n",
    "        async for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)\n",
    "            # Simulate some async processing\n",
    "            await asyncio.sleep(0.01)\n",
    "    \n",
    "    print(\"\\n\\n‚úÖ Async streaming completed!\")\n",
    "\n",
    "# Run the async example\n",
    "await async_stream_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming with System Messages and Multi-turn Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_turn_streaming():\n",
    "    \"\"\"Example of streaming in a multi-turn conversation\"\"\"\n",
    "    print(\"üí¨ Multi-turn streaming conversation...\\n\")\n",
    "    \n",
    "    # Conversation history\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I'm learning about machine learning. Can you explain what a neural network is?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"A neural network is a computational model inspired by biological neural networks. It consists of interconnected nodes (neurons) organized in layers that process information and learn patterns from data.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"That's helpful! Now can you explain how they learn, specifically about backpropagation?\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    with client.messages.stream(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=800,\n",
    "        system=\"You are a helpful AI tutor. Explain complex topics clearly with examples.\",\n",
    "        messages=conversation\n",
    "    ) as stream:\n",
    "        for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\n‚úÖ Multi-turn streaming completed!\")\n",
    "\n",
    "multi_turn_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling for Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_with_error_handling():\n",
    "    \"\"\"Example of proper error handling with streaming\"\"\"\n",
    "    print(\"üõ°Ô∏è Streaming with error handling...\\n\")\n",
    "    \n",
    "    try:\n",
    "        with client.messages.stream(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=300,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Tell me about the benefits of streaming API responses.\"\n",
    "                }\n",
    "            ]\n",
    "        ) as stream:\n",
    "            for text in stream.text_stream:\n",
    "                print(text, end=\"\", flush=True)\n",
    "                \n",
    "    except anthropic.APIError as e:\n",
    "        print(f\"‚ùå API Error: {e}\")\n",
    "    except anthropic.RateLimitError as e:\n",
    "        print(f\"‚è∞ Rate limit exceeded: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"üí• Unexpected error: {e}\")\n",
    "    else:\n",
    "        print(\"\\n\\n‚úÖ Streaming completed successfully!\")\n",
    "\n",
    "streaming_with_error_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Performance Comparison\n",
    "\n",
    "Let's compare the perceived performance between streaming and non-streaming responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_comparison():\n",
    "    \"\"\"Compare streaming vs non-streaming response times\"\"\"\n",
    "    prompt = \"Write a detailed explanation of how photosynthesis works, including the light and dark reactions.\"\n",
    "    \n",
    "    print(\"‚è±Ô∏è Performance Comparison\\n\")\n",
    "    \n",
    "    # Non-streaming request\n",
    "    print(\"1Ô∏è‚É£ Non-streaming request:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=800,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"   Total time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"   Response length: {len(response.content[0].text)} characters\")\n",
    "    print(f\"   Time to first token: {end_time - start_time:.2f} seconds\\n\")\n",
    "    \n",
    "    # Streaming request\n",
    "    print(\"2Ô∏è‚É£ Streaming request:\")\n",
    "    start_time = time.time()\n",
    "    first_token_time = None\n",
    "    char_count = 0\n",
    "    \n",
    "    with client.messages.stream(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=800,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    ) as stream:\n",
    "        for text in stream.text_stream:\n",
    "            if first_token_time is None:\n",
    "                first_token_time = time.time()\n",
    "            char_count += len(text)\n",
    "            # Don't print the actual text to keep output clean\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"   Total time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"   Response length: {char_count} characters\")\n",
    "    print(f\"   Time to first token: {first_token_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\nüìà Key benefit: Streaming provides much faster time-to-first-token!\")\n",
    "\n",
    "performance_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Chat Interface with Streaming\n",
    "\n",
    "Here's a practical example of how you might use streaming in a chat application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingChatBot:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "        self.client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "    \n",
    "    def add_user_message(self, message):\n",
    "        \"\"\"Add a user message to the conversation history\"\"\"\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "    \n",
    "    def get_streaming_response(self):\n",
    "        \"\"\"Get a streaming response from Claude\"\"\"\n",
    "        full_response = \"\"\n",
    "        \n",
    "        try:\n",
    "            with self.client.messages.stream(\n",
    "                model=\"claude-3-haiku-20240307\",\n",
    "                max_tokens=1000,\n",
    "                messages=self.conversation_history,\n",
    "                system=\"You are a helpful assistant. Be concise but thorough.\"\n",
    "            ) as stream:\n",
    "                print(\"ü§ñ Claude: \", end=\"\", flush=True)\n",
    "                for text in stream.text_stream:\n",
    "                    print(text, end=\"\", flush=True)\n",
    "                    full_response += text\n",
    "                print(\"\\n\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Add the response to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": full_response\n",
    "        })\n",
    "        \n",
    "        return full_response\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        \"\"\"Handle a single chat interaction\"\"\"\n",
    "        print(f\"üë§ You: {user_input}\")\n",
    "        self.add_user_message(user_input)\n",
    "        return self.get_streaming_response()\n",
    "\n",
    "# Demo the streaming chatbot\n",
    "print(\"üí¨ Streaming ChatBot Demo\\n\")\n",
    "chatbot = StreamingChatBot()\n",
    "\n",
    "# Simulate a conversation\n",
    "chatbot.chat(\"Hi! Can you help me understand what makes streaming useful for chatbots?\")\n",
    "chatbot.chat(\"That's interesting! How would I implement this in a web application?\")\n",
    "\n",
    "print(\"\\n‚úÖ ChatBot demo completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Streaming\n",
    "\n",
    "### When to Use Streaming\n",
    "- **Interactive applications**: Chat interfaces, real-time content generation\n",
    "- **Long responses**: When generating substantial amounts of text\n",
    "- **User experience**: When you want to show progress and reduce perceived latency\n",
    "\n",
    "### When NOT to Use Streaming\n",
    "- **Batch processing**: When processing multiple requests programmatically\n",
    "- **Short responses**: For brief answers where the overhead isn't worth it\n",
    "- **Data analysis**: When you need the complete response before processing\n",
    "\n",
    "### Implementation Tips\n",
    "1. **Always use context managers** (`with` statements) for proper resource cleanup\n",
    "2. **Handle errors gracefully** - network issues can interrupt streams\n",
    "3. **Buffer partial responses** if you need to process the complete text\n",
    "4. **Consider rate limits** - streaming requests count toward your rate limits\n",
    "5. **Test error scenarios** - what happens if the stream is interrupted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, you learned:\n",
    "\n",
    "- ‚úÖ How to use the `stream=True` parameter with `client.messages.stream()`\n",
    "- ‚úÖ Different streaming event types and how to handle them\n",
    "- ‚úÖ Async streaming for non-blocking operations\n",
    "- ‚úÖ Proper error handling for streaming requests\n",
    "- ‚úÖ Performance benefits of streaming (faster time-to-first-token)\n",
    "- ‚úÖ Building a practical streaming chat interface\n",
    "- ‚úÖ Best practices and when to use streaming\n",
    "\n",
    "Streaming is particularly powerful for creating responsive, interactive applications where users can see content being generated in real-time. The key advantage is the significantly reduced time-to-first-token, which makes applications feel much more responsive even if the total generation time is similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Modify the chatbot** to save conversation history to a file after each interaction\n",
    "2. **Create a streaming function** that generates a story paragraph by paragraph\n",
    "3. **Implement streaming with different models** and compare their response characteristics\n",
    "4. **Build error recovery** - if a stream is interrupted, retry the request\n",
    "5. **Create a streaming tokenizer** that counts tokens as they're received"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
