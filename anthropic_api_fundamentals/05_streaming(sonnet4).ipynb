{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Streaming Responses\n",
    "\n",
    "In this lesson, we'll learn how to stream responses from Claude using the Anthropic API. Streaming allows you to receive and display partial responses as they're generated, creating a more interactive user experience.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand when and why to use streaming\n",
    "- Learn how to implement streaming with the Anthropic Python SDK\n",
    "- Handle streaming responses properly\n",
    "- Implement error handling for streaming requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set up our API client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install anthropic python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Anthropic client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Streaming\n",
    "\n",
    "Let's start with a simple streaming example. The key is to use the `stream=True` parameter in your message creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_stream_example():\n",
    "    \"\"\"Basic streaming example\"\"\"\n",
    "    print(\"üöÄ Starting simple streaming example...\\n\")\n",
    "    \n",
    "    with client.messages.stream(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        max_tokens=1000,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Write a short story about a robot learning to paint.\"\n",
    "            }\n",
    "        ]\n",
    "    ) as stream:\n",
    "        for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\n‚úÖ Streaming completed!\")\n",
    "\n",
    "simple_stream_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Different Event Types\n",
    "\n",
    "When streaming, you can access different types of events and data. Let's explore the various event types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_stream_example():\n",
    "    \"\"\"Example showing different streaming events\"\"\"\n",
    "    print(\"üîç Detailed streaming example with event handling...\\n\")\n",
    "    \n",
    "    full_response = \"\"\n",
    "    \n",
    "    with client.messages.stream(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        max_tokens=500,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Explain quantum computing in simple terms.\"\n",
    "            }\n",
    "        ]\n",
    "    ) as stream:\n",
    "        # Handle the stream events\n",
    "        for event in stream:\n",
    "            if event.type == \"message_start\":\n",
    "                print(f\"üì® Message started - ID: {event.message.id}\")\n",
    "                print(f\"   Model: {event.message.model}\")\n",
    "                print(f\"   Usage: {event.message.usage}\\n\")\n",
    "                \n",
    "            elif event.type == \"content_block_start\":\n",
    "                print(f\"üìù Content block started - Type: {event.content_block.type}\\n\")\n",
    "                \n",
    "            elif event.type == \"content_block_delta\":\n",
    "                if hasattr(event.delta, 'text'):\n",
    "                    text_chunk = event.delta.text\n",
    "                    full_response += text_chunk\n",
    "                    print(text_chunk, end=\"\", flush=True)\n",
    "                    \n",
    "            elif event.type == \"content_block_stop\":\n",
    "                print(\"\\n\\nüìÑ Content block completed\")\n",
    "                \n",
    "            elif event.type == \"message_delta\":\n",
    "                if hasattr(event.delta, 'usage'):\n",
    "                    print(f\"\\nüìä Usage update: {event.delta.usage}\")\n",
    "                    \n",
    "            elif event.type == \"message_stop\":\n",
    "                print(\"\\nüèÅ Message completed\")\n",
    "    \n",
    "    print(f\"\\n\\nüìã Full response length: {len(full_response)} characters\")\n",
    "    print(\"‚úÖ Detailed streaming completed!\")\n",
    "\n",
    "detailed_stream_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Streaming\n",
    "\n",
    "For applications that need non-blocking operations, you can use async streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize async client\n",
    "async_client = anthropic.AsyncAnthropic(\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    ")\n",
    "\n",
    "async def async_stream_example():\n",
    "    \"\"\"Async streaming example\"\"\"\n",
    "    print(\"‚ö° Starting async streaming example...\\n\")\n",
    "    \n",
    "    async with async_client.messages.stream(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        max_tokens=500,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Write a haiku about streaming data.\"\n",
    "            }\n",
    "        ]\n",
    "    ) as stream:\n",
    "        async for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)\n",
    "            # Simulate some async processing\n",
    "            await asyncio.sleep(0.01)\n",
    "    \n",
    "    print(\"\\n\\n‚úÖ Async streaming completed!\")\n",
    "\n",
    "# Run the async example\n",
    "await async_stream_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming with System Messages and Multi-turn Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_turn_streaming():\n",
    "    \"\"\"Example of streaming in a multi-turn conversation\"\"\"\n",
    "    print(\"üí¨ Multi-turn streaming conversation...\\n\")\n",
    "    \n",
    "    # Conversation history\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I'm learning about machine learning. Can you explain what a neural network is?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"A neural network is a computational model inspired by biological neural networks. It consists of interconnected nodes (neurons) organized in layers that process information and learn patterns from data.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"That's helpful! Now can you explain how they learn, specifically about backpropagation?\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    with client.messages.stream(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        max_tokens=800,\n",
    "        system=\"You are a helpful AI tutor. Explain complex topics clearly with examples.\",\n",
    "        messages=conversation\n",
    "    ) as stream:\n",
    "        for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\n‚úÖ Multi-turn streaming completed!\")\n",
    "\n",
    "multi_turn_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling for Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_with_error_handling():\n",
    "    \"\"\"Example of proper error handling with streaming\"\"\"\n",
    "    print(\"üõ°Ô∏è Streaming with error handling...\\n\")\n",
    "    \n",
    "    try:\n",
    "        with client.messages.stream(\n",
    "            model=\"claude-3-sonnet-20240229\",\n",
    "            max_tokens=300,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Tell me about the benefits of streaming API responses.\"\n",
    "                }\n",
    "            ]\n",
    "        ) as stream:\n",
    "            for text in stream.text_stream:\n",
    "                print(text, end=\"\", flush=True)\n",
    "                \n",
    "    except anthropic.APIError as e:\n",
    "        print(f\"‚ùå API Error: {e}\")\n",
    "    except anthropic.RateLimitError as e:\n",
    "        print(f\"‚è∞ Rate limit exceeded: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"üí• Unexpected error: {e}\")\n",
    "    else:\n",
    "        print(\"\\n\\n‚úÖ Streaming completed successfully!\")\n",
    "\n",
    "streaming_with_error_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Performance Comparison\n",
    "\n",
    "Let's compare the perceived performance between streaming and non-streaming responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_comparison():\n",
    "    \"\"\"Compare streaming vs non-streaming response times\"\"\"\n",
    "    prompt = \"Write a detailed explanation of how photosynthesis works, including the light and dark reactions.\"\n",
    "    \n",
    "    print(\"‚è±Ô∏è Performance Comparison\\n\")\n",
    "    \n",
    "    # Non-streaming request\n",
    "    print(\"1Ô∏è‚É£ Non-streaming request:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        max_tokens=800,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"   Total time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"   Response length: {len(response.content[0].text)} characters\")\n",
    "    print(f\"   Time to first token: {end_time - start_time:.2f} seconds\\n\")\n",
    "    \n",
    "    # Streaming request\n",
    "    print(\"2Ô∏è‚É£ Streaming request:\")\n",
    "    start_time = time.time()\n",
    "    first_token_time = None\n",
    "    char_count = 0\n",
    "    \n",
    "    with client.messages.stream(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        max_tokens=800,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    ) as stream:\n",
    "        for text in stream.text_stream:\n",
    "            if first_token_time is None:\n",
    "                first_token_time = time.time()\n",
    "            char_count += len(text)\n",
    "            # Don't print the actual text to keep output clean\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"   Total time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"   Response length: {char_count} characters\")\n",
    "    print(f\"   Time to first token: {first_token_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\nüìà Key benefit: Streaming provides much faster time-to-first-token!\")\n",
    "\n",
    "performance_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Chat Interface with Streaming\n",
    "\n",
    "Here's a practical example of how you might use streaming in a chat application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingChatBot:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "        self.client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "    \n",
    "    def add_user_message(self, message):\n",
    "        \"\"\"Add a user message to the conversation history\"\"\"\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "    \n",
    "    def get_streaming_response(self):\n",
    "        \"\"\"Get a streaming response from Claude\"\"\"\n",
    "        full_response = \"\"\n",
    "        \n",
    "        try:\n",
    "            with self.client.messages.stream(\n",
    "                model=\"claude-3-sonnet-20240229\",\n",
    "                max_tokens=1000,\n",
    "                messages=self.conversation_history,\n",
    "                system=\"You are a helpful assistant. Be concise but thorough.\"\n",
    "            ) as stream:\n",
    "                print(\"ü§ñ Claude: \", end=\"\", flush=True)\n",
    "                for text in stream.text_stream:\n",
    "                    print(text, end=\"\", flush=True)\n",
    "                    full_response += text\n",
    "                print(\"\\n\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Add the response to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": full_response\n",
    "        })\n",
    "        \n",
    "        return full_response\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        \"\"\"Handle a single chat interaction\"\"\"\n",
    "        print(f\"üë§ You: {user_input}\")\n",
    "        self.add_user_message(user_input)\n",
    "        return self.get_streaming_response()\n",
    "\n",
    "# Demo the streaming chatbot\n",
    "print(\"üí¨ Streaming ChatBot Demo\\n\")\n",
    "chatbot = StreamingChatBot()\n",
    "\n",
    "# Simulate a conversation\n",
    "chatbot.chat(\"Hi! Can you help me understand what makes streaming useful for chatbots?\")\n",
    "chatbot.chat(\"That's interesting! How would I implement this in a web application?\")\n",
    "\n",
    "print(\"\\n‚úÖ ChatBot demo completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Streaming\n",
    "\n",
    "### When to Use Streaming\n",
    "- **Interactive applications**: Chat interfaces, real-time content generation\n",
    "- **Long responses**: When generating substantial amounts of text\n",
    "- **User experience**: When you want to show progress and reduce perceived latency\n",
    "\n",
    "### When NOT to Use Streaming\n",
    "- **Batch processing**: When processing multiple requests programmatically\n",
    "- **Short responses**: For brief answers where the overhead isn't worth it\n",
    "- **Data analysis**: When you need the complete response before processing\n",
    "\n",
    "### Implementation Tips\n",
    "1. **Always use context managers** (`with` statements) for proper resource cleanup\n",
    "2. **Handle errors gracefully** - network issues can interrupt streams\n",
    "3. **Buffer partial responses** if you need to process the complete text\n",
    "4. **Consider rate limits** - streaming requests count toward your rate limits\n",
    "5. **Test error scenarios** - what happens if the stream is interrupted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, you learned:\n",
    "\n",
    "- ‚úÖ How to use the `stream=True` parameter with `client.messages.stream()`\n",
    "- ‚úÖ Different streaming event types and how to handle them\n",
    "- ‚úÖ Async streaming for non-blocking operations\n",
    "- ‚úÖ Proper error handling for streaming requests\n",
    "- ‚úÖ Performance benefits of streaming (faster time-to-first-token)\n",
    "- ‚úÖ Building a practical streaming chat interface\n",
    "- ‚úÖ Best practices and when to use streaming\n",
    "\n",
    "Streaming is particularly powerful for creating responsive, interactive applications where users can see content being generated in real-time. The key advantage is the significantly reduced time-to-first-token, which makes applications feel much more responsive even if the total generation time is similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Modify the chatbot** to save conversation history to a file after each interaction\n",
    "2. **Create a streaming function** that generates a story paragraph by paragraph\n",
    "3. **Implement streaming with different models** and compare their response characteristics\n",
    "4. **Build error recovery** - if a stream is interrupted, retry the request\n",
    "5. **Create a streaming tokenizer** that counts tokens as they're received"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}